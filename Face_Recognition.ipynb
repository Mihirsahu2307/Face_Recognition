{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abb6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1b6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants:\n",
    "\n",
    "img_size = 160\n",
    "Data_DIR = \"C:\\\\Users\\\\USER\\\\Desktop\\\\ML-Projects\\\\Face_Recognition\\\\Data\" # Change to your directory\n",
    "Project_DIR = \"C:\\\\Users\\\\USER\\\\Desktop\\\\ML-Projects\\\\Face_Recognition\" # Change to your directory\n",
    "haarcascade_path = os.path.join(Project_DIR, \"haarcascade_frontalface_default.xml\")\n",
    "dlib_detector_path = os.path.join(Project_DIR, \"mmod_human_face_detector.dat\")\n",
    "facenet_path = os.path.join(Project_DIR, \"facenet_keras.h5\")\n",
    "database_path = os.path.join(Project_DIR, \"database.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60105f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "faceCascade = cv2.CascadeClassifier(haarcascade_path)\n",
    "detector = MTCNN()\n",
    "face_net = load_model(facenet_path)\n",
    "database = {}\n",
    "cropped_faces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925b0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(face):\n",
    "    face = cv2.resize(face, (img_size, img_size))\n",
    "    face = face.astype('float32')\n",
    "    mean, std = face.mean(), face.std()\n",
    "    face = (face - mean) / std\n",
    "    sample = np.expand_dims(face, axis=0)\n",
    "    embeddings = face_net.predict(sample)\n",
    "    return embeddings[0]\n",
    "\n",
    "def add_image_to_database(path, name):\n",
    "    img_array = cv2.imread(path)\n",
    "    faces = detector.detect_faces(img_array)\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]['box'] # expects atmost one face in the image\n",
    "        img_array = img_array[y : y + h, x : x + w]         \n",
    "        img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "        cropped_faces.append(img_array)\n",
    "        embedding = get_embedding(img_array)\n",
    "        database[name] = embedding\n",
    "\n",
    "def create_database():\n",
    "    for img in os.listdir(Data_DIR):\n",
    "        name = os.path.splitext(img)[0]\n",
    "        path = os.path.join(Data_DIR, img)\n",
    "        add_image_to_database(path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97f1fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # supresses tf warnings\n",
    "create_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7777961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(database))\n",
    "# print(len(cropped_faces))\n",
    "# for face in cropped_faces:\n",
    "#     plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d63a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save database:\n",
    "with open(database_path, 'wb') as handle:\n",
    "    pickle.dump(database, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead19d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load database:\n",
    "\n",
    "# print(len(database))\n",
    "# database = {}\n",
    "# print(len(database))\n",
    "with open(database_path, 'rb') as handle:\n",
    "    database = pickle.load(handle)\n",
    "    \n",
    "print(len(database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b692581",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(Project_DIR, \"haarcascade_frontalface_default.xml\")\n",
    "faceCascade = cv2.CascadeClassifier(path)\n",
    "font_scale = 1.5\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "# for strictness, valid_distance could be kept closer to 10. If the system is allowed to be lenient, it can be in btw 15 to 20\n",
    "valid_distance = 15  # Prediction is valid only if distance is less than or equal to valid_distance\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Face detection with haarcascade (faster):\n",
    "                       \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "    for x, y, w, h in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (200, 0, 0), 3)\n",
    "        embedding = get_embedding(frame[y : y + h, x : x + w])\n",
    "        min_dist = 1e5\n",
    "        name = \"Person\"\n",
    "        for person, embed in database.items():\n",
    "            L2 = np.linalg.norm(embed - embedding)\n",
    "            if(L2 < min_dist):\n",
    "                min_dist = L2\n",
    "                name = person\n",
    "    \n",
    "        if min_dist > valid_distance:\n",
    "            name = \"Unidentified\"\n",
    "        cv2.putText(frame, name, (x, y - 10), font, font_scale, (255, 255, 255), 2)\n",
    "        \n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "        \n",
    "# Face detection with mtcnn (slower):\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     faces = detector.detect_faces(frame)\n",
    "#     for face in faces:\n",
    "#         x, y, w, h = face['box']\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h), (200, 0, 0), 3)\n",
    "#         cv2.putText(frame, \"Person\", (x, y - 10), font, font_scale, (255, 255, 255), 2)\n",
    "\n",
    "#     cv2.imshow(\"Face Recognition\", frame)\n",
    "    \n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed716bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case webcam doesn't close properly, run this cell:\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
